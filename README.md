# Optimized_MNIST_by_hand

## Optimization Algorithms

### Momentum Gradient Descent

Plain gradient descent limits you from increasing the learning rate to avoid noise near the minimum, and divergence. Momentum reduces this noise and allows you to more safely apporach the min point, meaning you can safely increase the learning rate (to an extent). Momentum Gradient Descent almost always works faster than plain Gradient Descent

- Compute exponentially weighted averages of gradients
- update the weights and biases with these new gradients

Momentum


